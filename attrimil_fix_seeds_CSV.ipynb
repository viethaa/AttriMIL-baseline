{"cells":[{"cell_type":"code","execution_count":1,"id":"cell-0","metadata":{"id":"cell-0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768412727500,"user_tz":-420,"elapsed":24325,"user":{"displayName":"Viet Ha","userId":"14768171114431190159"}},"outputId":"fee7e561-b39b-4d1a-99bc-0f5b2fb55ae5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":2,"id":"cell-1","metadata":{"id":"cell-1","executionInfo":{"status":"ok","timestamp":1768412733448,"user_tz":-420,"elapsed":5945,"user":{"displayName":"Viet Ha","userId":"14768171114431190159"}}},"outputs":[],"source":["!pip install opencv-python h5py -q"]},{"cell_type":"code","execution_count":3,"id":"cell-2","metadata":{"id":"cell-2","executionInfo":{"status":"ok","timestamp":1768412756707,"user_tz":-420,"elapsed":23255,"user":{"displayName":"Viet Ha","userId":"14768171114431190159"}}},"outputs":[],"source":["import os\n","import h5py\n","import cv2\n","import random\n","import warnings\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","from tqdm import tqdm\n","from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from torchvision.models import ResNet50_Weights\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","SEED = 42\n","random.seed(SEED)\n","os.environ['PYTHONHASHSEED'] = str(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"]},{"cell_type":"code","execution_count":4,"id":"cell-3","metadata":{"id":"cell-3","executionInfo":{"status":"ok","timestamp":1768412756715,"user_tz":-420,"elapsed":4,"user":{"displayName":"Viet Ha","userId":"14768171114431190159"}}},"outputs":[],"source":["# Paths to CSV files\n","TRAIN_CSV = \"/content/drive/MyDrive/splits/train.csv\"\n","VAL_CSV = \"/content/drive/MyDrive/splits/val.csv\"\n","TEST_CSV = \"/content/drive/MyDrive/splits/test.csv\"\n","\n","CACHE_DIR = \"/content/drive/MyDrive/features_cache_paper\"\n","NUM_FRAMES = 50\n","NUM_CLASSES = 3\n","FEATURE_DIM = 512  # After first 3 blocks of ResNet50\n","\n","# Loss weights\n","ALPHA = 0.1\n","BETA = 0.001\n","\n","# Alternative reduced weights if training is unstable:\n","# ALPHA = 0.01   # Reduced spatial constraint\n","# BETA = 0.0001  # Reduced ranking constraint"]},{"cell_type":"code","execution_count":5,"id":"cell-4","metadata":{"id":"cell-4","executionInfo":{"status":"ok","timestamp":1768412756738,"user_tz":-420,"elapsed":11,"user":{"displayName":"Viet Ha","userId":"14768171114431190159"}}},"outputs":[],"source":["def replace_batchnorm_with_groupnorm(module, num_groups=32):\n","    \"\"\"\n","    Replace all BatchNorm layers with GroupNorm to avoid interactions between instances.\n","    \"\"\"\n","    for name, child in module.named_children():\n","        if isinstance(child, nn.BatchNorm2d):\n","            num_channels = child.num_features\n","            # Use num_groups=32 or adjust based on num_channels\n","            actual_groups = min(num_groups, num_channels)\n","            if num_channels % actual_groups != 0:\n","                actual_groups = 1  # Fallback to LayerNorm equivalent\n","            setattr(module, name, nn.GroupNorm(actual_groups, num_channels))\n","        else:\n","            replace_batchnorm_with_groupnorm(child, num_groups)\n","\n","def extract_video_frames(video_path, num_frames=50):\n","    \"\"\"Extract uniformly sampled frames from video\"\"\"\n","    cap = cv2.VideoCapture(video_path)\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","    if total_frames == 0:\n","        cap.release()\n","        return None\n","\n","    # Uniform sampling\n","    indices = np.linspace(0, total_frames-1, min(num_frames, total_frames), dtype=int)\n","\n","    frames = []\n","    for idx in indices:\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n","        ret, frame = cap.read()\n","        if ret:\n","            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            frames.append(frame)\n","\n","    cap.release()\n","    return frames if frames else None\n","\n","def extract_features_with_cache(video_path, cache_dir, feature_extractor, device, transform, overwrite=False):\n","    \"\"\"Extract ResNet50 (first 3 blocks) features with H5 caching\"\"\"\n","    os.makedirs(cache_dir, exist_ok=True)\n","\n","    # Cache path\n","    video_name = Path(video_path).stem\n","    class_name = Path(video_path).parent.name\n","    cache_subdir = os.path.join(cache_dir, class_name)\n","    os.makedirs(cache_subdir, exist_ok=True)\n","    cache_path = os.path.join(cache_subdir, f\"{video_name}.h5\")\n","\n","    # Load from cache\n","    if not overwrite and os.path.exists(cache_path):\n","        try:\n","            with h5py.File(cache_path, 'r') as f:\n","                features = torch.from_numpy(f['features'][:])\n","                coords = torch.from_numpy(f['coords'][:])\n","                nearest = eval(f['nearest'][()])\n","            return features, coords, nearest\n","        except:\n","            pass\n","\n","    # Extract frames\n","    frames = extract_video_frames(video_path, num_frames=NUM_FRAMES)\n","    if frames is None:\n","        return None, None, None\n","\n","    # Extract features\n","    features_list = []\n","    batch_size = 10\n","\n","    with torch.no_grad():\n","        for i in range(0, len(frames), batch_size):\n","            batch = [transform(f) for f in frames[i:i+batch_size]]\n","            batch = torch.stack(batch).to(device)\n","            feat = feature_extractor(batch)  # [B, 512, H, W]\n","            feat = F.adaptive_avg_pool2d(feat, (1, 1)).squeeze(-1).squeeze(-1)  # [B, 512]\n","            features_list.append(feat.cpu())\n","\n","    features = torch.cat(features_list, dim=0)  # [num_frames, 512]\n","\n","    # Create temporal coordinates\n","    num_frames_actual = features.shape[0]\n","    coords = torch.zeros(num_frames_actual, 2)\n","    coords[:, 0] = torch.arange(num_frames_actual)\n","\n","    # Nearest temporal neighbors\n","    nearest = []\n","    for i in range(num_frames_actual):\n","        neighbors = []\n","        if i > 0:\n","            neighbors.append(i - 1)\n","        neighbors.append(i)\n","        if i < num_frames_actual - 1:\n","            neighbors.append(i + 1)\n","        nearest.append(neighbors)\n","\n","    # Save to cache\n","    try:\n","        with h5py.File(cache_path, 'w') as f:\n","            f.create_dataset('features', data=features.numpy(), compression='gzip', compression_opts=4)\n","            f.create_dataset('coords', data=coords.numpy(), compression='gzip', compression_opts=4)\n","            f.create_dataset('nearest', data=str(nearest))\n","    except:\n","        pass\n","\n","    return features, coords, nearest"]},{"cell_type":"code","execution_count":6,"id":"cell-5","metadata":{"id":"cell-5","executionInfo":{"status":"ok","timestamp":1768412756742,"user_tz":-420,"elapsed":2,"user":{"displayName":"Viet Ha","userId":"14768171114431190159"}}},"outputs":[],"source":["class VideoMILDataset(Dataset):\n","    def __init__(self, video_paths, labels, cache_dir, feature_extractor, device, transform):\n","        self.video_paths = video_paths\n","        self.labels = labels\n","        self.cache_dir = cache_dir\n","        self.feature_extractor = feature_extractor\n","        self.device = device\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.video_paths)\n","\n","    def __getitem__(self, idx):\n","        video_path = self.video_paths[idx]\n","        label = self.labels[idx]\n","\n","        features, coords, nearest = extract_features_with_cache(\n","            video_path, self.cache_dir, self.feature_extractor,\n","            self.device, self.transform\n","        )\n","\n","        if features is None:\n","            features = torch.zeros(NUM_FRAMES, FEATURE_DIM)\n","            coords = torch.zeros(NUM_FRAMES, 2)\n","            nearest = [[i] for i in range(NUM_FRAMES)]\n","\n","        return features, label, coords, nearest"]},{"cell_type":"code","execution_count":7,"id":"cell-6","metadata":{"id":"cell-6","executionInfo":{"status":"ok","timestamp":1768412756806,"user_tz":-420,"elapsed":60,"user":{"displayName":"Viet Ha","userId":"14768171114431190159"}}},"outputs":[],"source":["# Constraint functions from AttriMIL paper\n","def spatial_constraint(attribute_score, coords, nearest_indices, n_classes):\n","    \"\"\"\n","    Spatial constraint loss to enforce smoothness in attribute scores.\n","    From paper: Encourages similar attribute scores for neighboring instances.\n","    FIXED: Added numerical stability and proper normalization\n","    \"\"\"\n","    if attribute_score.shape[2] == 0:\n","        return torch.tensor(0.0, device=attribute_score.device)\n","\n","    loss = 0.0\n","    num_instances = attribute_score.shape[2]\n","    count = 0\n","\n","    for i in range(num_instances):\n","        neighbors = nearest_indices[i]\n","        for neighbor_idx in neighbors:\n","            if neighbor_idx != i and neighbor_idx < num_instances:\n","                for c in range(n_classes):\n","                    # L2 distance between attribute scores\n","                    diff = attribute_score[0, c, i] - attribute_score[0, c, neighbor_idx]\n","                    loss += diff * diff\n","                    count += 1\n","\n","    # Normalize by actual number of comparisons\n","    if count > 0:\n","        return loss / count\n","    return torch.tensor(0.0, device=attribute_score.device)\n","\n","def ranking_constraint(attribute_score, label, n_classes):\n","    \"\"\"\n","    Ranking constraint loss to ensure correct class has higher attribute scores.\n","    From paper: Encourages attribute scores of positive class > negative classes.\n","    FIXED: Use bag-level aggregation instead of instance-level, reduced margin\n","    \"\"\"\n","    if attribute_score.shape[2] == 0:\n","        return torch.tensor(0.0, device=attribute_score.device)\n","\n","    target_class = label.item()\n","\n","    # Aggregate attribute scores to bag level (sum over instances)\n","    bag_scores = torch.sum(attribute_score[0], dim=1)  # Shape: [n_classes]\n","\n","    loss = 0.0\n","    # Encourage target class bag score > other class bag scores\n","    for c in range(n_classes):\n","        if c != target_class:\n","            # Smaller margin for stability\n","            margin = 0.1\n","            loss += F.relu(bag_scores[c] - bag_scores[target_class] + margin)\n","\n","    # Normalize by number of negative classes\n","    return loss / (n_classes - 1)"]},{"cell_type":"code","execution_count":8,"id":"cell-7","metadata":{"id":"cell-7","executionInfo":{"status":"ok","timestamp":1768412756807,"user_tz":-420,"elapsed":19,"user":{"displayName":"Viet Ha","userId":"14768171114431190159"}}},"outputs":[],"source":["class Attn_Net_Gated(nn.Module):\n","    def __init__(self, L=1024, D=256, dropout=False, n_classes=1):\n","        super(Attn_Net_Gated, self).__init__()\n","        self.attention_a = [\n","            nn.Linear(L, D),\n","            nn.Tanh()]\n","        self.attention_b = [nn.Linear(L, D), nn.Sigmoid()]\n","        if dropout:\n","            self.attention_a.append(nn.Dropout(0.25))\n","            self.attention_b.append(nn.Dropout(0.25))\n","        self.attention_a = nn.Sequential(*self.attention_a)\n","        self.attention_b = nn.Sequential(*self.attention_b)\n","        self.attention_c = nn.Linear(D, n_classes)\n","\n","    def forward(self, x):\n","        a = self.attention_a(x)\n","        b = self.attention_b(x)\n","        A = a.mul(b)\n","        A = self.attention_c(A)\n","        return A, x\n","\n","class AttriMIL(nn.Module):\n","    def __init__(self, n_classes=3, dim=512):\n","        super().__init__()\n","        self.adaptor = nn.Sequential(\n","            nn.Linear(dim, dim//2),\n","            nn.ReLU(),\n","            nn.Linear(dim//2, dim)\n","        )\n","\n","        attention = []\n","        classifiers = []\n","        for i in range(n_classes):\n","            attention.append(Attn_Net_Gated(L=dim, D=dim//2))\n","            classifiers.append(nn.Linear(dim, 1))\n","\n","        self.attention_nets = nn.ModuleList(attention)\n","        self.classifiers = nn.ModuleList(classifiers)\n","        self.n_classes = n_classes\n","        self.bias = nn.Parameter(torch.zeros(n_classes), requires_grad=True)\n","\n","    def forward(self, h):\n","        h = h + self.adaptor(h)\n","        A_raw = torch.empty(self.n_classes, h.size(0)).to(h.device)\n","        instance_score = torch.empty(1, self.n_classes, h.size(0)).float().to(h.device)\n","\n","        for c in range(self.n_classes):\n","            A, h_out = self.attention_nets[c](h)\n","            A = torch.transpose(A, 1, 0)\n","            A_raw[c] = A\n","            instance_score[0, c] = self.classifiers[c](h)[:, 0]\n","\n","\n","        attribute_score = torch.empty(1, self.n_classes, h.size(0)).float().to(h.device)\n","        logits = torch.empty(1, self.n_classes).float().to(h.device)\n","\n","        for c in range(self.n_classes):\n","            # Clamp attention scores to prevent explosion\n","            A_clamped = torch.clamp(A_raw[c], min=-10, max=10)\n","            exp_A = torch.exp(A_clamped)\n","\n","            # Compute attribute scores with clamped values\n","            attribute_score[0, c] = instance_score[0, c] * exp_A\n","\n","            # Add epsilon to prevent division by zero\n","            eps = 1e-8\n","            sum_exp_A = torch.sum(exp_A, dim=-1) + eps\n","\n","            # Compute logits\n","            logits[0, c] = torch.sum(attribute_score[0, c], keepdim=True, dim=-1) / sum_exp_A + self.bias[c]\n","\n","        Y_hat = torch.topk(logits, 1, dim=1)[1]\n","        Y_prob = F.softmax(logits, dim=1)\n","\n","        return logits, Y_prob, Y_hat, attribute_score"]},{"cell_type":"code","execution_count":9,"id":"cell-8","metadata":{"id":"cell-8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768412760552,"user_tz":-420,"elapsed":3748,"user":{"displayName":"Viet Ha","userId":"14768171114431190159"}},"outputId":"44a22f4a-383e-485b-dd12-957eb120d626"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:02<00:00, 46.5MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["âœ“ Feature extractor ready (First 3 blocks, BatchNormâ†’GroupNorm)\n","âœ“ Output feature dim: 512\n"]}],"source":["# Setup feature extractor\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","weights = ResNet50_Weights.IMAGENET1K_V1\n","resnet = models.resnet50(weights=weights)\n","\n","# Extract first 3 blocks (conv1, bn1, relu, maxpool, layer1, layer2, layer3)\n","# Output: 512 channels after layer3\n","feature_extractor = nn.Sequential(\n","    resnet.conv1,\n","    resnet.bn1,\n","    resnet.relu,\n","    resnet.maxpool,\n","    resnet.layer1,\n","    resnet.layer2,\n","    resnet.layer3\n",")\n","\n","# Replace BatchNorm with GroupNorm\n","replace_batchnorm_with_groupnorm(feature_extractor, num_groups=32)\n","\n","feature_extractor.eval()\n","feature_extractor = feature_extractor.to(device)\n","\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","print(\"âœ“ Feature extractor ready (First 3 blocks, BatchNormâ†’GroupNorm)\")\n","print(f\"âœ“ Output feature dim: {FEATURE_DIM}\")"]},{"cell_type":"code","execution_count":10,"id":"cell-9","metadata":{"id":"cell-9","colab":{"base_uri":"https://localhost:8080/","height":374},"executionInfo":{"status":"error","timestamp":1768412761278,"user_tz":-420,"elapsed":725,"user":{"displayName":"Viet Ha","userId":"14768171114431190159"}},"outputId":"4e2bb6ad-9c92-47de-ad8a-3ec441d0a3d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ“‚ Loading data splits from CSV files...\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/splits/train.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3849743351.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load train/val/test splits from CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ðŸ“‚ Loading data splits from CSV files...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_split_from_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_CSV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mval_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_split_from_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAL_CSV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtest_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_split_from_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_CSV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3849743351.py\u001b[0m in \u001b[0;36mload_split_from_csv\u001b[0;34m(csv_path, label_col, filename_col)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mCSV\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mvideo_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/splits/train.csv'"]}],"source":["# Load data from CSV files\n","def load_split_from_csv(csv_path, label_col='label', filename_col='filename'):\n","    \"\"\"\n","    Load video paths and labels from CSV file.\n","    CSV should have columns: filename, label\n","    \"\"\"\n","    df = pd.read_csv(csv_path)\n","    video_paths = df[filename_col].tolist()\n","    labels = df[label_col].tolist()\n","    return video_paths, labels\n","\n","# Load train/val/test splits from CSV\n","print(\"ðŸ“‚ Loading data splits from CSV files...\")\n","train_paths, train_labels = load_split_from_csv(TRAIN_CSV)\n","val_paths, val_labels = load_split_from_csv(VAL_CSV)\n","test_paths, test_labels = load_split_from_csv(TEST_CSV)\n","\n","print(f\"\\nTotal videos: {len(train_paths) + len(val_paths) + len(test_paths)}\")\n","print(f\"  Train: {len(train_paths)}\")\n","print(f\"  Val:   {len(val_paths)}\")\n","print(f\"  Test:  {len(test_paths)}\")\n","\n","# Count class distribution\n","label_names = {0: 'Normal', 1: 'Adenoma', 2: 'Malignant'}\n","print(f\"\\nClass distribution:\")\n","print(f\"Train - Normal: {train_labels.count(0)}, Adenoma: {train_labels.count(1)}, Malignant: {train_labels.count(2)}\")\n","print(f\"Val   - Normal: {val_labels.count(0)}, Adenoma: {val_labels.count(1)}, Malignant: {val_labels.count(2)}\")\n","print(f\"Test  - Normal: {test_labels.count(0)}, Adenoma: {test_labels.count(1)}, Malignant: {test_labels.count(2)}\")"]},{"cell_type":"code","execution_count":null,"id":"cell-10","metadata":{"id":"cell-10","executionInfo":{"status":"aborted","timestamp":1768412761275,"user_tz":-420,"elapsed":58265,"user":{"displayName":"Viet Ha","userId":"14768171114431190159"}}},"outputs":[],"source":["# Pre extract all features\n","print(\"\\n Pre-extracting features for all videos...\")\n","print(\"First 3 ResNet blocks with GroupNorm\")\n","\n","\n","all_paths = train_paths + val_paths + test_paths\n","success_count = 0\n","failed_videos = []\n","\n","for video_path in tqdm(all_paths, desc=\"Extracting features\"):\n","    result = extract_features_with_cache(video_path, CACHE_DIR, feature_extractor, device, transform)\n","    if result[0] is not None:\n","        success_count += 1\n","    else:\n","        failed_videos.append(video_path)\n","\n","print(f\"\\nâœ“ Feature extraction complete!\")\n","print(f\"  Successfully cached: {success_count}/{len(all_paths)} videos\")\n","if failed_videos:\n","    print(f\"  Failed: {len(failed_videos)} videos\")\n","print(f\"\\nFeatures saved to: {CACHE_DIR}\")"]},{"cell_type":"code","execution_count":null,"id":"cell-11","metadata":{"id":"cell-11","executionInfo":{"status":"aborted","timestamp":1768412761331,"user_tz":-420,"elapsed":58321,"user":{"displayName":"Viet Ha","userId":"14768171114431190159"}}},"outputs":[],"source":["# Create datasets and loaders\n","train_dataset = VideoMILDataset(train_paths, train_labels, CACHE_DIR, feature_extractor, device, transform)\n","val_dataset = VideoMILDataset(val_paths, val_labels, CACHE_DIR, feature_extractor, device, transform)\n","\n","# batch_size=1\n","train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)\n","val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)\n","\n","print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)}\")"]},{"cell_type":"code","execution_count":null,"id":"cell-12","metadata":{"id":"cell-12","executionInfo":{"status":"aborted","timestamp":1768412761332,"user_tz":-420,"elapsed":58322,"user":{"displayName":"Viet Ha","userId":"14768171114431190159"}}},"outputs":[],"source":["# Initialize model\n","model = AttriMIL(n_classes=NUM_CLASSES, dim=FEATURE_DIM).to(device)\n","\n","# Paper hyperparameters: lr=2e-4, weight_decay from AdamW\n","optimizer = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-5)\n","criterion = nn.CrossEntropyLoss()\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=150, eta_min=1e-6)\n","\n","print(\"âœ“ Model initialized\")\n","print(f\"âœ“ Loss weights: Î±={ALPHA} (spatial), Î²={BETA} (ranking)\")"]},{"cell_type":"code","execution_count":null,"id":"cell-13","metadata":{"id":"cell-13","executionInfo":{"status":"aborted","timestamp":1768412761333,"user_tz":-420,"elapsed":58322,"user":{"displayName":"Viet Ha","userId":"14768171114431190159"}}},"outputs":[],"source":["# Training loop with spatial and ranking constraints\n","train_losses, val_losses, train_accs, val_accs, train_f1s, val_f1s = [], [], [], [], [], []\n","train_bag_losses, train_spa_losses, train_rank_losses = [], [], []\n","best_acc = 0\n","\n","for epoch in range(150):\n","    # Train\n","    model.train()\n","    train_loss = 0\n","    train_bag_loss = 0\n","    train_spa_loss = 0\n","    train_rank_loss = 0\n","    train_preds = []\n","    train_true = []\n","    nan_count = 0\n","\n","    for features, label, coords, nearest in tqdm(train_loader, desc=f\"Epoch {epoch+1}/150 [Train]\"):\n","        features = features.squeeze(0).to(device)  # Remove batch dim: [1, T, D] -> [T, D]\n","        label = label.to(device)\n","        coords = coords.squeeze(0)  # [T, 2]\n","        nearest = nearest[0]  # List of neighbor indices\n","\n","        optimizer.zero_grad()\n","\n","        logits, probs, pred, attr_scores = model(features)\n","\n","        # Bag-level loss (standard cross-entropy)\n","        loss_bag = criterion(logits, label)\n","\n","        # Spatial constraint loss (smoothness between neighbors)\n","        loss_spa = spatial_constraint(attr_scores, coords, nearest, NUM_CLASSES)\n","\n","        # Ranking constraint loss (positive class > negative classes)\n","        loss_rank = ranking_constraint(attr_scores, label, NUM_CLASSES)\n","\n","        # Total loss with paper's weights: Î±=0.1, Î²=0.001\n","        loss = loss_bag + ALPHA * loss_spa + BETA * loss_rank\n","\n","\n","        if torch.isnan(loss) or torch.isinf(loss):\n","            nan_count += 1\n","            print(f\"  Warning: NaN/Inf loss detected (bag={loss_bag.item():.4f}, spa={loss_spa.item():.4f}, rank={loss_rank.item():.4f})\")\n","            continue\n","\n","        loss.backward()\n","\n","        #Gradient clipping\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        train_bag_loss += loss_bag.item()\n","        train_spa_loss += loss_spa.item()\n","        train_rank_loss += loss_rank.item()\n","        train_preds.append(pred.item())\n","        train_true.append(label.item())\n","\n","    # Handle division by zero if all batches had NaN\n","    num_valid_batches = len(train_loader) - nan_count\n","    if num_valid_batches == 0:\n","        print(f\"Epoch {epoch+1}: All batches had NaN losses! Stopping training.\")\n","        break\n","\n","    train_loss /= num_valid_batches\n","    train_bag_loss /= num_valid_batches\n","    train_spa_loss /= num_valid_batches\n","    train_rank_loss /= num_valid_batches\n","    train_acc = accuracy_score(train_true, train_preds)\n","    train_f1 = f1_score(train_true, train_preds, average='macro')\n","\n","    # Validation (no constraints during evaluation)\n","    model.eval()\n","    val_loss = 0\n","    val_preds = []\n","    val_true = []\n","\n","    with torch.no_grad():\n","        for features, label, coords, nearest in tqdm(val_loader, desc=f\"Epoch {epoch+1}/150 [Val]\"):\n","            features = features.squeeze(0).to(device)\n","            label = label.to(device)\n","\n","            logits, probs, pred, attr_scores = model(features)\n","            loss = criterion(logits, label)\n","\n","            # Skip NaN losses in validation too\n","            if not (torch.isnan(loss) or torch.isinf(loss)):\n","                val_loss += loss.item()\n","                val_preds.append(pred.item())\n","                val_true.append(label.item())\n","\n","    val_loss /= len(val_loader)\n","    val_acc = accuracy_score(val_true, val_preds)\n","    val_f1 = f1_score(val_true, val_preds, average='macro')\n","\n","    scheduler.step()\n","\n","    print(f\"Epoch {epoch+1}: Train Acc={train_acc:.4f}, F1={train_f1:.4f}, Loss={train_loss:.4f} (Bag={train_bag_loss:.4f}, Spa={train_spa_loss:.4f}, Rank={train_rank_loss:.4f}) | Val Acc={val_acc:.4f}, F1={val_f1:.4f}, Loss={val_loss:.4f}\")\n","    if nan_count > 0:\n","        print(f\"  (Skipped {nan_count} batches due to NaN)\")\n","\n","    if val_acc > best_acc:\n","        best_acc = val_acc\n","        torch.save(model.state_dict(), \"/content/drive/MyDrive/attrimil_paper_best_model.pth\")\n","        print(f\"  âœ“ Best model saved (Val Acc: {best_acc:.4f})\")\n","\n","    torch.save(model.state_dict(), \"/content/drive/MyDrive/attrimil_paper_current_model.pth\")\n","\n","    train_losses.append(train_loss)\n","    val_losses.append(val_loss)\n","    train_accs.append(train_acc)\n","    val_accs.append(val_acc)\n","    train_f1s.append(train_f1)\n","    val_f1s.append(val_f1)\n","    train_bag_losses.append(train_bag_loss)\n","    train_spa_losses.append(train_spa_loss)\n","    train_rank_losses.append(train_rank_loss)\n","\n","print(f\"\\nBest validation accuracy: {best_acc:.4f}\")"]},{"cell_type":"code","execution_count":null,"id":"cell-14","metadata":{"id":"cell-14","executionInfo":{"status":"aborted","timestamp":1768412761333,"user_tz":-420,"elapsed":58322,"user":{"displayName":"Viet Ha","userId":"14768171114431190159"}}},"outputs":[],"source":["print(f\"Best Val Acc: {np.array(val_accs).max():.4f}\")"]},{"cell_type":"code","execution_count":null,"id":"cell-15","metadata":{"id":"cell-15","executionInfo":{"status":"aborted","timestamp":1768412761349,"user_tz":-420,"elapsed":58338,"user":{"displayName":"Viet Ha","userId":"14768171114431190159"}}},"outputs":[],"source":["epochs = range(len(train_losses))\n","\n","plt.figure(figsize=(20, 4))\n","\n","# Plotting Losses\n","plt.subplot(1, 4, 1)\n","plt.plot(epochs, train_losses, 'r', label='Training Loss (Total)')\n","plt.plot(epochs, val_losses, 'b', label='Validation Loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","# Plotting Loss Components\n","plt.subplot(1, 4, 2)\n","plt.plot(epochs, train_bag_losses, 'g', label='Bag Loss')\n","plt.plot(epochs, train_spa_losses, 'orange', label=f'Spatial Loss (Î±={ALPHA})')\n","plt.plot(epochs, train_rank_losses, 'purple', label=f'Ranking Loss (Î²={BETA})')\n","plt.title('Training Loss Components')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","# Plotting Accuracies\n","plt.subplot(1, 4, 3)\n","plt.plot(epochs, train_accs, 'r', label='Training Accuracy')\n","plt.plot(epochs, val_accs, 'b', label='Validation Accuracy')\n","plt.title('Training and Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","# Plotting F1 Scores\n","plt.subplot(1, 4, 4)\n","plt.plot(epochs, train_f1s, 'r', label='Training F1 Score')\n","plt.plot(epochs, val_f1s, 'b', label='Validation F1 Score')\n","plt.title('Training and Validation F1 Score')\n","plt.xlabel('Epochs')\n","plt.ylabel('F1 Score')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.savefig('/content/drive/MyDrive/attrimil_paper_results.png', dpi=300, bbox_inches='tight')\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}